{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibFfYbyXTNe9"
      },
      "source": [
        "# Week 8: Language modeling!\n",
        "\n",
        "# Rasika Bhalerao\n",
        "\n",
        "# Agenda\n",
        "\n",
        "- Language models\n",
        "- BERT for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGIrvi_6TJf-",
        "outputId": "0ac879fd-0db1-4812-9588-19366b4b6719"
      },
      "source": [
        "# !pip install transformers\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM, BertForSequenceClassification, AdamW, logging\n",
        "from torch.nn import functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "  device = torch.device(\"cuda\")\n",
        "  print('Using GPU ', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print('Using CPU')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlR6JmX6fk-x"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhRMHHe5XBC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0044ec96-5af7-4719-ebc1-54f30d7aa26c"
      },
      "source": [
        "sentence = 'Feature normalization produces features with mean [MASK] and variance 1.'\n",
        "# sentence = '[MASK] have one question [MASK] you Amy.'\n",
        "# sentence = 'Maddy do you want to [MASK] your announcement?'\n",
        "# sentence = 'one two three [MASK] five'\n",
        "\n",
        "\n",
        "mask_id = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "token_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
        "output = model(token_ids)[0].squeeze(0)\n",
        "\n",
        "for mask_idx in (token_ids==mask_id)[0].nonzero():\n",
        "  hs = output[mask_idx.item()]\n",
        "  log_probs = torch.nn.LogSoftmax(dim=0)(hs)\n",
        "  best_guess = tokenizer.convert_ids_to_tokens(torch.argmax(log_probs).item())\n",
        "  print(best_guess)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2ZYlqZkgjrH"
      },
      "source": [
        "### BERT for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MUfgF9rV614"
      },
      "source": [
        "# Hyperparameter tuning\n",
        "learning_rates = [1e-3, 1e-4, 1e-5]\n",
        "batch_sizes = [8, 16, 32] # unused with fake data\n",
        "max_epochs = 100\n",
        "early_stop_epochs = 3\n",
        "epsilon = 1e-5\n",
        "\n",
        "losses = {} # dict from loss (float) to array of hyperparameter values"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD2GK51lWUO8"
      },
      "source": [
        "# Training set\n",
        "example_batch_of_text = ['whiskers tail tail paw purr', 'meow whiskers whiskers', 'paw woof bark bark']\n",
        "example_batch_of_labels = torch.tensor([1,1,0])\n",
        "train_text_batches = [example_batch_of_text]\n",
        "train_label_batches = [example_batch_of_labels]\n",
        "\n",
        "# Dev set\n",
        "example_batch_of_text = ['paw bark woof bark', 'meow meow paw purr', 'henlo whiskers']\n",
        "example_batch_of_labels = torch.tensor([0,1,1])\n",
        "dev_text_batches = [example_batch_of_text]\n",
        "dev_label_batches = [example_batch_of_labels]\n",
        "\n",
        "# Test set\n",
        "example_batch_of_text = ['bark paw paw paw', 'meow whiskers purr', 'woof woof woof']\n",
        "example_batch_of_labels = torch.tensor([0,1,0])\n",
        "test_text_batches = [example_batch_of_text]\n",
        "test_label_batches = [example_batch_of_labels]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaCxgrALWWZq",
        "outputId": "f6ab10a0-e7fe-403c-8995-655c334e8355"
      },
      "source": [
        "# Hyperparameter tuning\n",
        "\n",
        "for lr in learning_rates:\n",
        "\n",
        "  # train using selected hyperparameter\n",
        "  model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True)\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "  nondecreasing_epochs = early_stop_epochs\n",
        "  prev_loss = None\n",
        "  for epoch in range(max_epochs):\n",
        "    total_loss = 0\n",
        "    for text_batch, label_batch in zip(train_text_batches, train_label_batches):\n",
        "      encoding = tokenizer(text_batch, \n",
        "                          return_tensors='pt', padding=True, \n",
        "                          truncation=True, max_length=256)\n",
        "      input_ids = encoding['input_ids']\n",
        "      attention_mask = encoding['attention_mask']\n",
        "\n",
        "      outputs = model(input_ids, attention_mask=attention_mask)\n",
        "      loss = F.cross_entropy(outputs.logits, label_batch)\n",
        "\n",
        "      total_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    print(f'Train loss epoch {epoch}: {total_loss}')\n",
        "    if prev_loss is not None and prev_loss - epsilon <= total_loss:\n",
        "      nondecreasing_epochs -= 1\n",
        "    else:\n",
        "      nondecreasing_epochs = early_stop_epochs\n",
        "    prev_loss = total_loss\n",
        "    if nondecreasing_epochs <= 0:\n",
        "      break\n",
        "  \n",
        "  # check dev loss with current model\n",
        "  total_loss = 0\n",
        "  for text_batch, label_batch in zip(dev_text_batches, dev_label_batches):\n",
        "    encoding = tokenizer(text_batch, \n",
        "                        return_tensors='pt', padding=True, \n",
        "                        truncation=True, max_length=256)\n",
        "    input_ids = encoding['input_ids']\n",
        "    attention_mask = encoding['attention_mask']\n",
        "\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    loss = F.cross_entropy(outputs.logits, label_batch)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "  print(f'Dev loss: {total_loss}')\n",
        "  losses[total_loss] = [lr]"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss epoch 0: 0.675462007522583\n",
            "Train loss epoch 1: 0.7045699954032898\n",
            "Train loss epoch 2: 0.4964850842952728\n",
            "Train loss epoch 3: 0.6509253978729248\n",
            "Train loss epoch 4: 1.9392576217651367\n",
            "Train loss epoch 5: 3.7950007915496826\n",
            "Dev loss: 0.9086148738861084\n",
            "Train loss epoch 0: 0.789372444152832\n",
            "Train loss epoch 1: 0.5797554850578308\n",
            "Train loss epoch 2: 0.4332538843154907\n",
            "Train loss epoch 3: 0.4039294421672821\n",
            "Train loss epoch 4: 0.5009769201278687\n",
            "Train loss epoch 5: 0.3650988042354584\n",
            "Train loss epoch 6: 0.29423579573631287\n",
            "Train loss epoch 7: 0.2196759134531021\n",
            "Train loss epoch 8: 0.17826694250106812\n",
            "Train loss epoch 9: 0.16204044222831726\n",
            "Train loss epoch 10: 0.1118406280875206\n",
            "Train loss epoch 11: 0.10373073816299438\n",
            "Train loss epoch 12: 0.06387980282306671\n",
            "Train loss epoch 13: 0.03434162214398384\n",
            "Train loss epoch 14: 0.012304903008043766\n",
            "Train loss epoch 15: 0.0088256960734725\n",
            "Train loss epoch 16: 0.005327882710844278\n",
            "Train loss epoch 17: 0.004738897085189819\n",
            "Train loss epoch 18: 0.0029354149010032415\n",
            "Train loss epoch 19: 0.0021114808041602373\n",
            "Train loss epoch 20: 0.0018690060824155807\n",
            "Train loss epoch 21: 0.0012116743018850684\n",
            "Train loss epoch 22: 0.0009497164282947779\n",
            "Train loss epoch 23: 0.0009024321916513145\n",
            "Train loss epoch 24: 0.0005544984596781433\n",
            "Train loss epoch 25: 0.00039628674858249724\n",
            "Train loss epoch 26: 0.0003409154014661908\n",
            "Train loss epoch 27: 0.00023703857732471079\n",
            "Train loss epoch 28: 0.0003358720859978348\n",
            "Train loss epoch 29: 0.00031895088613964617\n",
            "Train loss epoch 30: 0.00045827726717107\n",
            "Train loss epoch 31: 0.010376261547207832\n",
            "Train loss epoch 32: 0.00013886793749406934\n",
            "Train loss epoch 33: 8.244951459346339e-05\n",
            "Train loss epoch 34: 0.9273752570152283\n",
            "Train loss epoch 35: 0.00010120342631125823\n",
            "Train loss epoch 36: 8.233047992689535e-05\n",
            "Train loss epoch 37: 4.618445873260498\n",
            "Train loss epoch 38: 2.852785348892212\n",
            "Train loss epoch 39: 7.398510206257924e-05\n",
            "Train loss epoch 40: 7.2633039962966e-05\n",
            "Train loss epoch 41: 3.151038254145533e-05\n",
            "Train loss epoch 42: 5.54695725440979e-05\n",
            "Train loss epoch 43: 5.964145020698197e-05\n",
            "Train loss epoch 44: 4.211970008327626e-05\n",
            "Train loss epoch 45: 5.642318501486443e-05\n",
            "Train loss epoch 46: 0.00012244989920873195\n",
            "Train loss epoch 47: 6.047570423106663e-05\n",
            "Train loss epoch 48: 5.503292413777672e-05\n",
            "Train loss epoch 49: 5.550992500502616e-05\n",
            "Train loss epoch 50: 6.86614221194759e-05\n",
            "Dev loss: 9.57179581746459e-05\n",
            "Train loss epoch 0: 0.6736447215080261\n",
            "Train loss epoch 1: 0.7905133366584778\n",
            "Train loss epoch 2: 0.6169073581695557\n",
            "Train loss epoch 3: 0.5337048172950745\n",
            "Train loss epoch 4: 0.48871996998786926\n",
            "Train loss epoch 5: 0.4271356761455536\n",
            "Train loss epoch 6: 0.46916136145591736\n",
            "Train loss epoch 7: 0.48587536811828613\n",
            "Train loss epoch 8: 0.3655122220516205\n",
            "Train loss epoch 9: 0.524655282497406\n",
            "Train loss epoch 10: 0.32849469780921936\n",
            "Train loss epoch 11: 0.36810413002967834\n",
            "Train loss epoch 12: 0.44137370586395264\n",
            "Train loss epoch 13: 0.388090580701828\n",
            "Train loss epoch 14: 0.38141191005706787\n",
            "Train loss epoch 15: 0.36772868037223816\n",
            "Train loss epoch 16: 0.3346520960330963\n",
            "Train loss epoch 17: 0.36777663230895996\n",
            "Train loss epoch 18: 0.3365124464035034\n",
            "Train loss epoch 19: 0.2871929407119751\n",
            "Train loss epoch 20: 0.30306383967399597\n",
            "Train loss epoch 21: 0.30952832102775574\n",
            "Train loss epoch 22: 0.26909199357032776\n",
            "Train loss epoch 23: 0.27209901809692383\n",
            "Train loss epoch 24: 0.3126552402973175\n",
            "Train loss epoch 25: 0.22223997116088867\n",
            "Train loss epoch 26: 0.26621881127357483\n",
            "Train loss epoch 27: 0.3155491054058075\n",
            "Train loss epoch 28: 0.2721567451953888\n",
            "Train loss epoch 29: 0.3575810194015503\n",
            "Train loss epoch 30: 0.21999724209308624\n",
            "Train loss epoch 31: 0.2378532439470291\n",
            "Train loss epoch 32: 0.22955714166164398\n",
            "Train loss epoch 33: 0.1930251121520996\n",
            "Train loss epoch 34: 0.18678100407123566\n",
            "Train loss epoch 35: 0.1338760405778885\n",
            "Train loss epoch 36: 0.14912785589694977\n",
            "Train loss epoch 37: 0.1576344221830368\n",
            "Train loss epoch 38: 0.12150158733129501\n",
            "Train loss epoch 39: 0.13109052181243896\n",
            "Train loss epoch 40: 0.13671164214611053\n",
            "Train loss epoch 41: 0.13560231029987335\n",
            "Train loss epoch 42: 0.12984250485897064\n",
            "Train loss epoch 43: 0.12361345440149307\n",
            "Train loss epoch 44: 0.12402603030204773\n",
            "Train loss epoch 45: 0.11480659991502762\n",
            "Train loss epoch 46: 0.14416055381298065\n",
            "Train loss epoch 47: 0.10813496261835098\n",
            "Train loss epoch 48: 0.12077703326940536\n",
            "Train loss epoch 49: 0.09675873070955276\n",
            "Train loss epoch 50: 0.1031041219830513\n",
            "Train loss epoch 51: 0.09105368703603745\n",
            "Train loss epoch 52: 0.09324686974287033\n",
            "Train loss epoch 53: 0.06268084794282913\n",
            "Train loss epoch 54: 0.08997410535812378\n",
            "Train loss epoch 55: 0.06964214891195297\n",
            "Train loss epoch 56: 0.06166910007596016\n",
            "Train loss epoch 57: 0.06535216420888901\n",
            "Train loss epoch 58: 0.05535256490111351\n",
            "Train loss epoch 59: 0.051584213972091675\n",
            "Train loss epoch 60: 0.0454726405441761\n",
            "Train loss epoch 61: 0.0392315648496151\n",
            "Train loss epoch 62: 0.04399240389466286\n",
            "Train loss epoch 63: 0.044869426637887955\n",
            "Train loss epoch 64: 0.059703413397073746\n",
            "Dev loss: 0.033148448914289474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrfmaODSWbfX",
        "outputId": "865ce83b-fda3-47e8-c10e-108f04bb4146"
      },
      "source": [
        "# Train model with best hyperparameter\n",
        "\n",
        "best_hyperparams = losses[min(losses)]\n",
        "best_lr = best_hyperparams[0]\n",
        "print(f'Best lr: {best_lr}')\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True)\n",
        "model.to(device)\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
        "\n",
        "nondecreasing_epochs = early_stop_epochs\n",
        "prev_loss = None\n",
        "for epoch in range(max_epochs):\n",
        "  total_loss = 0\n",
        "  for text_batch, label_batch in zip(train_text_batches, train_label_batches):\n",
        "    encoding = tokenizer(text_batch, \n",
        "                        return_tensors='pt', padding=True, \n",
        "                        truncation=True, max_length=256)\n",
        "    input_ids = encoding['input_ids']\n",
        "    attention_mask = encoding['attention_mask']\n",
        "\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    loss = F.cross_entropy(outputs.logits, label_batch)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f'Train loss epoch {epoch}: {total_loss}')\n",
        "  if prev_loss is not None and prev_loss - epsilon <= total_loss:\n",
        "    nondecreasing_epochs -= 1\n",
        "  prev_loss = total_loss\n",
        "  if nondecreasing_epochs <= 0:\n",
        "    break\n"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best lr: 0.0001\n",
            "Train loss epoch 0: 0.7076751589775085\n",
            "Train loss epoch 1: 0.5344696640968323\n",
            "Train loss epoch 2: 0.23449377715587616\n",
            "Train loss epoch 3: 0.17738954722881317\n",
            "Train loss epoch 4: 0.15177451074123383\n",
            "Train loss epoch 5: 0.12609608471393585\n",
            "Train loss epoch 6: 0.11721354722976685\n",
            "Train loss epoch 7: 0.08962824940681458\n",
            "Train loss epoch 8: 0.07122112065553665\n",
            "Train loss epoch 9: 0.049825847148895264\n",
            "Train loss epoch 10: 0.02504490502178669\n",
            "Train loss epoch 11: 0.013294324278831482\n",
            "Train loss epoch 12: 0.013333950191736221\n",
            "Train loss epoch 13: 0.00651288777589798\n",
            "Train loss epoch 14: 0.008425845764577389\n",
            "Train loss epoch 15: 0.005788961425423622\n",
            "Train loss epoch 16: 0.006442824844270945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChrS0ShqWuQJ",
        "outputId": "1af1268d-e128-4720-a03a-5f0456e097c9"
      },
      "source": [
        "# Get F1 score of trained model on test set\n",
        "\n",
        "output_labels = torch.tensor([])\n",
        "for text_batch, label_batch in zip(test_text_batches, test_label_batches):\n",
        "  encoding = tokenizer(text_batch, \n",
        "                      return_tensors='pt', padding=True, \n",
        "                      truncation=True, max_length=256)\n",
        "  input_ids = encoding['input_ids']\n",
        "  attention_mask = encoding['attention_mask']\n",
        "\n",
        "  outputs = model(input_ids, attention_mask=attention_mask)\n",
        "  predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
        "  output_labels = torch.cat([output_labels, predicted_labels], dim=0)\n",
        "\n",
        "\n",
        "correct_labels = torch.cat(test_label_batches, dim=0)\n",
        "f1 = f1_score(correct_labels, output_labels)\n",
        "print(f'F1 score: {f1}')"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAC5YIsEtqiv",
        "outputId": "4e0ed685-141c-4a3d-9681-3849c4fc3cb5"
      },
      "source": [
        "outputs.logits"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9954, -3.4220],\n",
              "        [-1.9956,  2.9089],\n",
              "        [ 2.3230, -3.7223]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP_RpToV0Dqr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
