{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtBmyDgq2gYA"
      },
      "source": [
        "# Week 6: Gradient Descent\n",
        "\n",
        "# Rasika Bhalerao\n",
        "\n",
        "# Agenda\n",
        "\n",
        "- Gradient descent in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1Y62oVF2ams"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnEeOsSg39I1"
      },
      "source": [
        "### Pytorch is powerful!\n",
        "- Open source Python package for machine learning\n",
        "- Easy to use with GPU (or CPU)\n",
        "- Computation is done with tensors\n",
        "- You can think of it as Numpy with extra capabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wwOmmtB4djE"
      },
      "source": [
        "### Pytorch basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0JVKXm54S3Q"
      },
      "source": [
        "# Some data\n",
        "x = torch.tensor([3.])\n",
        "y = torch.tensor([1.])\n",
        "\n",
        "# Initialize some weights\n",
        "a = torch.tensor([4.], requires_grad=True)\n",
        "b = torch.tensor([5.], requires_grad=True)\n",
        "\n",
        "# Define model and loss\n",
        "y_hat = a * x + b\n",
        "loss = (y - y_hat)**2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1k1mid94fsb",
        "outputId": "08b17ff3-5ec0-41cf-b7ea-2ec0d7e1b730"
      },
      "source": [
        "print(x)\n",
        "print(y)\n",
        "print(a)\n",
        "print(b)\n",
        "print(y_hat)\n",
        "print(loss)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3.])\n",
            "tensor([1.])\n",
            "tensor([4.], requires_grad=True)\n",
            "tensor([5.], requires_grad=True)\n",
            "tensor([17.], grad_fn=<AddBackward0>)\n",
            "tensor([256.], grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUfGuyYu4job",
        "outputId": "44c45614-819f-4ed1-8185-8d2a2647535a"
      },
      "source": [
        "print(a.grad)\n",
        "print(b.grad)\n",
        "loss.backward() # this tells it to actually calculate the gradient\n",
        "print(a.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "tensor([96.])\n",
            "tensor([32.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eer-bygd4l6r",
        "outputId": "4c6d44a2-acb7-4bbd-a643-8c368f952800"
      },
      "source": [
        "# This would be one iteration of updating the weights using the gradient\n",
        "a = a - 0.01 * a.grad\n",
        "b = b - 0.01 * b.grad\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3.0400], grad_fn=<SubBackward0>)\n",
            "tensor([4.6800], grad_fn=<SubBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc3u4BKA4pUr",
        "outputId": "db3065c3-4507-49e6-fe89-02d9e51db0c0"
      },
      "source": [
        "y_hat = a * x + b\n",
        "loss = (y - y_hat)**2\n",
        "print(y_hat) # 13.8, closer to the target 1 than it was before (17)\n",
        "print(loss) # 163.8, smaller than it was before (256)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([13.8000], grad_fn=<AddBackward0>)\n",
            "tensor([163.8400], grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-1YVtkE42v-"
      },
      "source": [
        "### Classification via (Stochastic) Gradient Descent in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7JrzZ974uRs"
      },
      "source": [
        "# Some data\n",
        "x = torch.rand(50, 3)          # 50 rows, 3 features\n",
        "y = torch.randint(0, 3, (50,)) # 50 rows, 3 possible categories\n",
        "\n",
        "x_train = x[:40]\n",
        "x_test = x[40:]\n",
        "y_train = y[:40]\n",
        "y_test = y[40:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDAuXTp_4-It"
      },
      "source": [
        "# DataLoader\n",
        "batch_size = 5 # it will iterate through with batches of 5 rows\n",
        "\n",
        "train_dataset = [[x_train[i], y_train[i]] for i in range(len(x_train))]\n",
        "test_dataset = [[x_test[i], y_test[i]] for i in range(len(x_test))]\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vrRmaIx5Bts",
        "outputId": "97c5dba6-452a-46e3-83cf-84efc1d041e8"
      },
      "source": [
        "# First step of training: implementing epochs\n",
        "epochs = 300 # maximum epochs\n",
        "early_stop_epochs = 3 # stop iterating when loss doesn't decrease for 3 epochs\n",
        "prev_loss = 9999999\n",
        "\n",
        "nondecreasing = 0\n",
        "for epoch in range(epochs):\n",
        "    loss = torch.rand(1) # for now it is random - implement below!\n",
        "    print(f'Epoch {epoch} loss: {loss.item()}')\n",
        "    \n",
        "    if prev_loss - loss <= 1e-3:\n",
        "        nondecreasing += 1\n",
        "    else:\n",
        "        nondecreasing = 0\n",
        "    \n",
        "    if nondecreasing >= early_stop_epochs:\n",
        "        break\n",
        "        \n",
        "    prev_loss = loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 0.4107988476753235\n",
            "Epoch 1 loss: 0.5948238968849182\n",
            "Epoch 2 loss: 0.20232146978378296\n",
            "Epoch 3 loss: 0.7460334300994873\n",
            "Epoch 4 loss: 0.12138360738754272\n",
            "Epoch 5 loss: 0.8712044358253479\n",
            "Epoch 6 loss: 0.694099485874176\n",
            "Epoch 7 loss: 0.45883089303970337\n",
            "Epoch 8 loss: 0.985586941242218\n",
            "Epoch 9 loss: 0.6581254005432129\n",
            "Epoch 10 loss: 0.6086974740028381\n",
            "Epoch 11 loss: 0.06593376398086548\n",
            "Epoch 12 loss: 0.8101882338523865\n",
            "Epoch 13 loss: 0.02086097002029419\n",
            "Epoch 14 loss: 0.9142488837242126\n",
            "Epoch 15 loss: 0.14593613147735596\n",
            "Epoch 16 loss: 0.4454752802848816\n",
            "Epoch 17 loss: 0.5019964575767517\n",
            "Epoch 18 loss: 0.9360269904136658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXmoCC2h5FX9",
        "outputId": "6669243f-ee94-4db3-8b47-74970a1e70b3"
      },
      "source": [
        "# Second step of training: add batches\n",
        "\n",
        "epochs = 300\n",
        "early_stop_epochs = 3\n",
        "prev_loss = 9999999\n",
        "\n",
        "nondecreasing = 0\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    #### start of code added for batches ####\n",
        "    total_loss = 0\n",
        "    for [batch_x, batch_y] in train_loader:\n",
        "        \n",
        "        batch_loss = 0\n",
        "        for x, y in zip(batch_x, batch_y):\n",
        "            batch_loss += torch.rand(1)\n",
        "        total_loss += batch_loss\n",
        "        # This is where we would do a backwards pass with batch_loss\n",
        "        \n",
        "    #### end of code added for batches ####\n",
        "\n",
        "    print(f'Epoch {epoch} loss: {total_loss.item()}')\n",
        "    \n",
        "    if prev_loss - total_loss <= 1e-3:\n",
        "        nondecreasing += 1\n",
        "    else:\n",
        "        nondecreasing = 0\n",
        "    \n",
        "    if nondecreasing >= early_stop_epochs:\n",
        "        break\n",
        "        \n",
        "    prev_loss = total_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 21.56624412536621\n",
            "Epoch 1 loss: 21.683334350585938\n",
            "Epoch 2 loss: 22.559980392456055\n",
            "Epoch 3 loss: 20.69846534729004\n",
            "Epoch 4 loss: 21.97745132446289\n",
            "Epoch 5 loss: 17.83310317993164\n",
            "Epoch 6 loss: 19.787073135375977\n",
            "Epoch 7 loss: 17.591415405273438\n",
            "Epoch 8 loss: 18.504169464111328\n",
            "Epoch 9 loss: 21.17566680908203\n",
            "Epoch 10 loss: 18.670026779174805\n",
            "Epoch 11 loss: 20.925670623779297\n",
            "Epoch 12 loss: 16.878910064697266\n",
            "Epoch 13 loss: 19.584733963012695\n",
            "Epoch 14 loss: 16.968475341796875\n",
            "Epoch 15 loss: 18.290822982788086\n",
            "Epoch 16 loss: 19.382844924926758\n",
            "Epoch 17 loss: 18.766212463378906\n",
            "Epoch 18 loss: 18.053171157836914\n",
            "Epoch 19 loss: 22.089073181152344\n",
            "Epoch 20 loss: 20.175006866455078\n",
            "Epoch 21 loss: 22.43926239013672\n",
            "Epoch 22 loss: 15.905424118041992\n",
            "Epoch 23 loss: 18.494518280029297\n",
            "Epoch 24 loss: 20.5318603515625\n",
            "Epoch 25 loss: 16.755041122436523\n",
            "Epoch 26 loss: 18.540491104125977\n",
            "Epoch 27 loss: 19.663827896118164\n",
            "Epoch 28 loss: 19.87942123413086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dkXKz5b5O3e",
        "outputId": "911abe7e-557a-4ec7-bcc5-580d0e2c2ea0"
      },
      "source": [
        "# Third step of training: use actual model loss\n",
        "lr = 0.01 # learning rate\n",
        "weights = torch.nn.Linear(3, 3) # initialize weights for 3 features, 3 outputs\n",
        "softmax_fn = torch.nn.Softmax(dim=1) # this returns a function that does softmax\n",
        "loss_fn = torch.nn.functional.cross_entropy # loss function, takes input and target arrays, returns numerical loss\n",
        "optimizer = torch.optim.SGD(weights.parameters(), lr=lr) # this will do SGD for us\n",
        "\n",
        "\n",
        "epochs = 300\n",
        "early_stop_epochs = 4\n",
        "prev_loss = 9999999\n",
        "\n",
        "nondecreasing = 0\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    total_loss = 0\n",
        "    for [batch_x, batch_y] in train_loader:\n",
        "        \n",
        "        #### start of code added for model ####\n",
        "        optimizer.zero_grad() # forget gradients from previous iteration\n",
        "        output = weights(batch_x) # multiplication part of model\n",
        "        softmaxed_output = softmax_fn(output) # softmax part of model\n",
        "        batch_loss = loss_fn(softmaxed_output, batch_y) # calculate loss\n",
        "        \n",
        "        total_loss += batch_loss # for record keeping\n",
        "        \n",
        "        batch_loss.backward() # calculate gradients\n",
        "        optimizer.step() # do the step with lr\n",
        "\n",
        "        #### end of code added for model ####\n",
        "        \n",
        "    print(f'Epoch {epoch} loss: {total_loss.item()}')\n",
        "    \n",
        "    if prev_loss - total_loss <= 1e-3:\n",
        "        nondecreasing += 1\n",
        "    else:\n",
        "        nondecreasing = 0\n",
        "    \n",
        "    if nondecreasing >= early_stop_epochs:\n",
        "        break\n",
        "        \n",
        "    prev_loss = total_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 8.959355354309082\n",
            "Epoch 1 loss: 8.952441215515137\n",
            "Epoch 2 loss: 8.945649147033691\n",
            "Epoch 3 loss: 8.938977241516113\n",
            "Epoch 4 loss: 8.932424545288086\n",
            "Epoch 5 loss: 8.92598819732666\n",
            "Epoch 6 loss: 8.91966724395752\n",
            "Epoch 7 loss: 8.913461685180664\n",
            "Epoch 8 loss: 8.907366752624512\n",
            "Epoch 9 loss: 8.901384353637695\n",
            "Epoch 10 loss: 8.89551067352295\n",
            "Epoch 11 loss: 8.88974380493164\n",
            "Epoch 12 loss: 8.88408374786377\n",
            "Epoch 13 loss: 8.878527641296387\n",
            "Epoch 14 loss: 8.873072624206543\n",
            "Epoch 15 loss: 8.867718696594238\n",
            "Epoch 16 loss: 8.862462997436523\n",
            "Epoch 17 loss: 8.857304573059082\n",
            "Epoch 18 loss: 8.852241516113281\n",
            "Epoch 19 loss: 8.847271919250488\n",
            "Epoch 20 loss: 8.842394828796387\n",
            "Epoch 21 loss: 8.837606430053711\n",
            "Epoch 22 loss: 8.832908630371094\n",
            "Epoch 23 loss: 8.828295707702637\n",
            "Epoch 24 loss: 8.823768615722656\n",
            "Epoch 25 loss: 8.819324493408203\n",
            "Epoch 26 loss: 8.814961433410645\n",
            "Epoch 27 loss: 8.81067943572998\n",
            "Epoch 28 loss: 8.806475639343262\n",
            "Epoch 29 loss: 8.802349090576172\n",
            "Epoch 30 loss: 8.798297882080078\n",
            "Epoch 31 loss: 8.794320106506348\n",
            "Epoch 32 loss: 8.79041576385498\n",
            "Epoch 33 loss: 8.786581039428711\n",
            "Epoch 34 loss: 8.782814979553223\n",
            "Epoch 35 loss: 8.779118537902832\n",
            "Epoch 36 loss: 8.775487899780273\n",
            "Epoch 37 loss: 8.77192211151123\n",
            "Epoch 38 loss: 8.768421173095703\n",
            "Epoch 39 loss: 8.764981269836426\n",
            "Epoch 40 loss: 8.761603355407715\n",
            "Epoch 41 loss: 8.758285522460938\n",
            "Epoch 42 loss: 8.755024909973145\n",
            "Epoch 43 loss: 8.751822471618652\n",
            "Epoch 44 loss: 8.748676300048828\n",
            "Epoch 45 loss: 8.745584487915039\n",
            "Epoch 46 loss: 8.742546081542969\n",
            "Epoch 47 loss: 8.739561080932617\n",
            "Epoch 48 loss: 8.736625671386719\n",
            "Epoch 49 loss: 8.733741760253906\n",
            "Epoch 50 loss: 8.730907440185547\n",
            "Epoch 51 loss: 8.728120803833008\n",
            "Epoch 52 loss: 8.725380897521973\n",
            "Epoch 53 loss: 8.722686767578125\n",
            "Epoch 54 loss: 8.720039367675781\n",
            "Epoch 55 loss: 8.717434883117676\n",
            "Epoch 56 loss: 8.714872360229492\n",
            "Epoch 57 loss: 8.71235466003418\n",
            "Epoch 58 loss: 8.70987606048584\n",
            "Epoch 59 loss: 8.707439422607422\n",
            "Epoch 60 loss: 8.705041885375977\n",
            "Epoch 61 loss: 8.702683448791504\n",
            "Epoch 62 loss: 8.700362205505371\n",
            "Epoch 63 loss: 8.698078155517578\n",
            "Epoch 64 loss: 8.695831298828125\n",
            "Epoch 65 loss: 8.693619728088379\n",
            "Epoch 66 loss: 8.691442489624023\n",
            "Epoch 67 loss: 8.689298629760742\n",
            "Epoch 68 loss: 8.687188148498535\n",
            "Epoch 69 loss: 8.685111045837402\n",
            "Epoch 70 loss: 8.683066368103027\n",
            "Epoch 71 loss: 8.681050300598145\n",
            "Epoch 72 loss: 8.679067611694336\n",
            "Epoch 73 loss: 8.677114486694336\n",
            "Epoch 74 loss: 8.675189971923828\n",
            "Epoch 75 loss: 8.673294067382812\n",
            "Epoch 76 loss: 8.671426773071289\n",
            "Epoch 77 loss: 8.669587135314941\n",
            "Epoch 78 loss: 8.667773246765137\n",
            "Epoch 79 loss: 8.665987968444824\n",
            "Epoch 80 loss: 8.664226531982422\n",
            "Epoch 81 loss: 8.662491798400879\n",
            "Epoch 82 loss: 8.660780906677246\n",
            "Epoch 83 loss: 8.659093856811523\n",
            "Epoch 84 loss: 8.657431602478027\n",
            "Epoch 85 loss: 8.655792236328125\n",
            "Epoch 86 loss: 8.654176712036133\n",
            "Epoch 87 loss: 8.652582168579102\n",
            "Epoch 88 loss: 8.651010513305664\n",
            "Epoch 89 loss: 8.649459838867188\n",
            "Epoch 90 loss: 8.647930145263672\n",
            "Epoch 91 loss: 8.646421432495117\n",
            "Epoch 92 loss: 8.644932746887207\n",
            "Epoch 93 loss: 8.643464088439941\n",
            "Epoch 94 loss: 8.64201545715332\n",
            "Epoch 95 loss: 8.640584945678711\n",
            "Epoch 96 loss: 8.63917350769043\n",
            "Epoch 97 loss: 8.63778018951416\n",
            "Epoch 98 loss: 8.636404037475586\n",
            "Epoch 99 loss: 8.63504695892334\n",
            "Epoch 100 loss: 8.633707046508789\n",
            "Epoch 101 loss: 8.632384300231934\n",
            "Epoch 102 loss: 8.63107681274414\n",
            "Epoch 103 loss: 8.629786491394043\n",
            "Epoch 104 loss: 8.628512382507324\n",
            "Epoch 105 loss: 8.627253532409668\n",
            "Epoch 106 loss: 8.62601089477539\n",
            "Epoch 107 loss: 8.624783515930176\n",
            "Epoch 108 loss: 8.62356948852539\n",
            "Epoch 109 loss: 8.622370719909668\n",
            "Epoch 110 loss: 8.621187210083008\n",
            "Epoch 111 loss: 8.620016098022461\n",
            "Epoch 112 loss: 8.61885929107666\n",
            "Epoch 113 loss: 8.617716789245605\n",
            "Epoch 114 loss: 8.616586685180664\n",
            "Epoch 115 loss: 8.615469932556152\n",
            "Epoch 116 loss: 8.61436653137207\n",
            "Epoch 117 loss: 8.613275527954102\n",
            "Epoch 118 loss: 8.61219596862793\n",
            "Epoch 119 loss: 8.611128807067871\n",
            "Epoch 120 loss: 8.610074043273926\n",
            "Epoch 121 loss: 8.609031677246094\n",
            "Epoch 122 loss: 8.60799789428711\n",
            "Epoch 123 loss: 8.606978416442871\n",
            "Epoch 124 loss: 8.605968475341797\n",
            "Epoch 125 loss: 8.60496997833252\n",
            "Epoch 126 loss: 8.603983879089355\n",
            "Epoch 127 loss: 8.603006362915039\n",
            "Epoch 128 loss: 8.602039337158203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQqdqcA65VaN",
        "outputId": "92ae9bb2-21b8-478f-ad14-99f323882a3e"
      },
      "source": [
        "# Last part of training: print F1 score every epoch\n",
        "lr = 0.01\n",
        "weights = torch.nn.Linear(3, 3) # 3 features, 3 outputs\n",
        "softmax_fn = torch.nn.Softmax(dim=1) # this returns a function that does softmax\n",
        "loss_fn = torch.nn.functional.cross_entropy # this is a function that takes parameters input and target\n",
        "optimizer = torch.optim.SGD(weights.parameters(), lr=lr) # this will do SGD for us\n",
        "\n",
        "\n",
        "epochs = 300\n",
        "early_stop_epochs = 4\n",
        "prev_loss = 9999999\n",
        "\n",
        "nondecreasing = 0\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    total_loss = 0\n",
        "    y_pred = None\n",
        "    for [batch_x, batch_y] in train_loader:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = weights(batch_x)\n",
        "        softmaxed_output = softmax_fn(output)\n",
        "        batch_loss = loss_fn(softmaxed_output, batch_y)\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #### start of code added for F1 ####\n",
        "        _, predicted_labels = torch.max(softmaxed_output.data, 1)\n",
        "        if y_pred is not None:\n",
        "            y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
        "        else:\n",
        "            y_pred = predicted_labels\n",
        "    \n",
        "    f1 = f1_score(y_train, y_pred, average='micro')\n",
        "    #### end of code added for F1 ####\n",
        "    print(f'Epoch {epoch} loss: {total_loss.item()}, F1: {f1}')\n",
        "    \n",
        "    if prev_loss - total_loss <= 1e-5:\n",
        "        nondecreasing += 1\n",
        "    else:\n",
        "        nondecreasing = 0\n",
        "    \n",
        "    if nondecreasing >= early_stop_epochs:\n",
        "        break\n",
        "        \n",
        "    prev_loss = total_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 8.806382179260254, F1: 0.35\n",
            "Epoch 1 loss: 8.801663398742676, F1: 0.325\n",
            "Epoch 2 loss: 8.79702091217041, F1: 0.35\n",
            "Epoch 3 loss: 8.792455673217773, F1: 0.35\n",
            "Epoch 4 loss: 8.787962913513184, F1: 0.325\n",
            "Epoch 5 loss: 8.783543586730957, F1: 0.325\n",
            "Epoch 6 loss: 8.779196739196777, F1: 0.35\n",
            "Epoch 7 loss: 8.774919509887695, F1: 0.35\n",
            "Epoch 8 loss: 8.770710945129395, F1: 0.4000000000000001\n",
            "Epoch 9 loss: 8.766570091247559, F1: 0.4000000000000001\n",
            "Epoch 10 loss: 8.762496948242188, F1: 0.45\n",
            "Epoch 11 loss: 8.7584867477417, F1: 0.4000000000000001\n",
            "Epoch 12 loss: 8.754541397094727, F1: 0.4000000000000001\n",
            "Epoch 13 loss: 8.75065803527832, F1: 0.4000000000000001\n",
            "Epoch 14 loss: 8.746835708618164, F1: 0.4000000000000001\n",
            "Epoch 15 loss: 8.743074417114258, F1: 0.375\n",
            "Epoch 16 loss: 8.739371299743652, F1: 0.375\n",
            "Epoch 17 loss: 8.735727310180664, F1: 0.375\n",
            "Epoch 18 loss: 8.732138633728027, F1: 0.375\n",
            "Epoch 19 loss: 8.728606224060059, F1: 0.375\n",
            "Epoch 20 loss: 8.725129127502441, F1: 0.4000000000000001\n",
            "Epoch 21 loss: 8.721704483032227, F1: 0.4000000000000001\n",
            "Epoch 22 loss: 8.718334197998047, F1: 0.4000000000000001\n",
            "Epoch 23 loss: 8.715012550354004, F1: 0.4000000000000001\n",
            "Epoch 24 loss: 8.71174430847168, F1: 0.4000000000000001\n",
            "Epoch 25 loss: 8.708524703979492, F1: 0.4000000000000001\n",
            "Epoch 26 loss: 8.705352783203125, F1: 0.425\n",
            "Epoch 27 loss: 8.702229499816895, F1: 0.425\n",
            "Epoch 28 loss: 8.699153900146484, F1: 0.425\n",
            "Epoch 29 loss: 8.696124076843262, F1: 0.45\n",
            "Epoch 30 loss: 8.693138122558594, F1: 0.45\n",
            "Epoch 31 loss: 8.690196990966797, F1: 0.45\n",
            "Epoch 32 loss: 8.687299728393555, F1: 0.45\n",
            "Epoch 33 loss: 8.68444538116455, F1: 0.45\n",
            "Epoch 34 loss: 8.681632995605469, F1: 0.45\n",
            "Epoch 35 loss: 8.678860664367676, F1: 0.45\n",
            "Epoch 36 loss: 8.676130294799805, F1: 0.47500000000000003\n",
            "Epoch 37 loss: 8.67343807220459, F1: 0.47500000000000003\n",
            "Epoch 38 loss: 8.670785903930664, F1: 0.47500000000000003\n",
            "Epoch 39 loss: 8.668171882629395, F1: 0.47500000000000003\n",
            "Epoch 40 loss: 8.665594100952148, F1: 0.47500000000000003\n",
            "Epoch 41 loss: 8.663055419921875, F1: 0.47500000000000003\n",
            "Epoch 42 loss: 8.660550117492676, F1: 0.47500000000000003\n",
            "Epoch 43 loss: 8.658082962036133, F1: 0.47500000000000003\n",
            "Epoch 44 loss: 8.655649185180664, F1: 0.47500000000000003\n",
            "Epoch 45 loss: 8.653250694274902, F1: 0.47500000000000003\n",
            "Epoch 46 loss: 8.650884628295898, F1: 0.47500000000000003\n",
            "Epoch 47 loss: 8.648552894592285, F1: 0.47500000000000003\n",
            "Epoch 48 loss: 8.64625358581543, F1: 0.47500000000000003\n",
            "Epoch 49 loss: 8.643985748291016, F1: 0.47500000000000003\n",
            "Epoch 50 loss: 8.641749382019043, F1: 0.47500000000000003\n",
            "Epoch 51 loss: 8.639544486999512, F1: 0.47500000000000003\n",
            "Epoch 52 loss: 8.637369155883789, F1: 0.47500000000000003\n",
            "Epoch 53 loss: 8.635224342346191, F1: 0.47500000000000003\n",
            "Epoch 54 loss: 8.633108139038086, F1: 0.47500000000000003\n",
            "Epoch 55 loss: 8.631020545959473, F1: 0.5\n",
            "Epoch 56 loss: 8.628962516784668, F1: 0.5\n",
            "Epoch 57 loss: 8.626931190490723, F1: 0.5\n",
            "Epoch 58 loss: 8.62492847442627, F1: 0.5\n",
            "Epoch 59 loss: 8.622952461242676, F1: 0.5\n",
            "Epoch 60 loss: 8.621001243591309, F1: 0.525\n",
            "Epoch 61 loss: 8.619077682495117, F1: 0.525\n",
            "Epoch 62 loss: 8.617178916931152, F1: 0.525\n",
            "Epoch 63 loss: 8.61530590057373, F1: 0.525\n",
            "Epoch 64 loss: 8.613457679748535, F1: 0.525\n",
            "Epoch 65 loss: 8.61163330078125, F1: 0.525\n",
            "Epoch 66 loss: 8.609832763671875, F1: 0.525\n",
            "Epoch 67 loss: 8.608057022094727, F1: 0.525\n",
            "Epoch 68 loss: 8.606303215026855, F1: 0.525\n",
            "Epoch 69 loss: 8.604572296142578, F1: 0.525\n",
            "Epoch 70 loss: 8.602864265441895, F1: 0.525\n",
            "Epoch 71 loss: 8.601178169250488, F1: 0.525\n",
            "Epoch 72 loss: 8.59951400756836, F1: 0.55\n",
            "Epoch 73 loss: 8.597870826721191, F1: 0.525\n",
            "Epoch 74 loss: 8.596248626708984, F1: 0.525\n",
            "Epoch 75 loss: 8.594648361206055, F1: 0.5\n",
            "Epoch 76 loss: 8.593067169189453, F1: 0.47500000000000003\n",
            "Epoch 77 loss: 8.591506958007812, F1: 0.45\n",
            "Epoch 78 loss: 8.589964866638184, F1: 0.45\n",
            "Epoch 79 loss: 8.588443756103516, F1: 0.45\n",
            "Epoch 80 loss: 8.586941719055176, F1: 0.45\n",
            "Epoch 81 loss: 8.585457801818848, F1: 0.45\n",
            "Epoch 82 loss: 8.583993911743164, F1: 0.45\n",
            "Epoch 83 loss: 8.582545280456543, F1: 0.45\n",
            "Epoch 84 loss: 8.581116676330566, F1: 0.45\n",
            "Epoch 85 loss: 8.579706192016602, F1: 0.45\n",
            "Epoch 86 loss: 8.578311920166016, F1: 0.45\n",
            "Epoch 87 loss: 8.576934814453125, F1: 0.45\n",
            "Epoch 88 loss: 8.575575828552246, F1: 0.45\n",
            "Epoch 89 loss: 8.57423210144043, F1: 0.45\n",
            "Epoch 90 loss: 8.572906494140625, F1: 0.45\n",
            "Epoch 91 loss: 8.571595191955566, F1: 0.45\n",
            "Epoch 92 loss: 8.57030200958252, F1: 0.45\n",
            "Epoch 93 loss: 8.569022178649902, F1: 0.45\n",
            "Epoch 94 loss: 8.567758560180664, F1: 0.45\n",
            "Epoch 95 loss: 8.566510200500488, F1: 0.45\n",
            "Epoch 96 loss: 8.565277099609375, F1: 0.47500000000000003\n",
            "Epoch 97 loss: 8.564058303833008, F1: 0.47500000000000003\n",
            "Epoch 98 loss: 8.562853813171387, F1: 0.47500000000000003\n",
            "Epoch 99 loss: 8.561664581298828, F1: 0.47500000000000003\n",
            "Epoch 100 loss: 8.560487747192383, F1: 0.47500000000000003\n",
            "Epoch 101 loss: 8.559326171875, F1: 0.47500000000000003\n",
            "Epoch 102 loss: 8.558177947998047, F1: 0.47500000000000003\n",
            "Epoch 103 loss: 8.557042121887207, F1: 0.45\n",
            "Epoch 104 loss: 8.555920600891113, F1: 0.45\n",
            "Epoch 105 loss: 8.554813385009766, F1: 0.45\n",
            "Epoch 106 loss: 8.553717613220215, F1: 0.45\n",
            "Epoch 107 loss: 8.552635192871094, F1: 0.45\n",
            "Epoch 108 loss: 8.551563262939453, F1: 0.425\n",
            "Epoch 109 loss: 8.550506591796875, F1: 0.425\n",
            "Epoch 110 loss: 8.549460411071777, F1: 0.425\n",
            "Epoch 111 loss: 8.548426628112793, F1: 0.425\n",
            "Epoch 112 loss: 8.547405242919922, F1: 0.425\n",
            "Epoch 113 loss: 8.546393394470215, F1: 0.425\n",
            "Epoch 114 loss: 8.545393943786621, F1: 0.4000000000000001\n",
            "Epoch 115 loss: 8.54440689086914, F1: 0.4000000000000001\n",
            "Epoch 116 loss: 8.54343032836914, F1: 0.4000000000000001\n",
            "Epoch 117 loss: 8.542463302612305, F1: 0.375\n",
            "Epoch 118 loss: 8.541508674621582, F1: 0.375\n",
            "Epoch 119 loss: 8.54056453704834, F1: 0.375\n",
            "Epoch 120 loss: 8.539630889892578, F1: 0.4000000000000001\n",
            "Epoch 121 loss: 8.53870677947998, F1: 0.4000000000000001\n",
            "Epoch 122 loss: 8.53779411315918, F1: 0.4000000000000001\n",
            "Epoch 123 loss: 8.536890029907227, F1: 0.4000000000000001\n",
            "Epoch 124 loss: 8.53599739074707, F1: 0.4000000000000001\n",
            "Epoch 125 loss: 8.535114288330078, F1: 0.4000000000000001\n",
            "Epoch 126 loss: 8.534239768981934, F1: 0.4000000000000001\n",
            "Epoch 127 loss: 8.53337574005127, F1: 0.4000000000000001\n",
            "Epoch 128 loss: 8.532520294189453, F1: 0.4000000000000001\n",
            "Epoch 129 loss: 8.5316743850708, F1: 0.4000000000000001\n",
            "Epoch 130 loss: 8.530838012695312, F1: 0.4000000000000001\n",
            "Epoch 131 loss: 8.530009269714355, F1: 0.4000000000000001\n",
            "Epoch 132 loss: 8.529191017150879, F1: 0.4000000000000001\n",
            "Epoch 133 loss: 8.52838134765625, F1: 0.4000000000000001\n",
            "Epoch 134 loss: 8.527579307556152, F1: 0.4000000000000001\n",
            "Epoch 135 loss: 8.526785850524902, F1: 0.4000000000000001\n",
            "Epoch 136 loss: 8.5260009765625, F1: 0.4000000000000001\n",
            "Epoch 137 loss: 8.525224685668945, F1: 0.4000000000000001\n",
            "Epoch 138 loss: 8.524456024169922, F1: 0.4000000000000001\n",
            "Epoch 139 loss: 8.52369499206543, F1: 0.4000000000000001\n",
            "Epoch 140 loss: 8.522942543029785, F1: 0.4000000000000001\n",
            "Epoch 141 loss: 8.522197723388672, F1: 0.4000000000000001\n",
            "Epoch 142 loss: 8.52146053314209, F1: 0.4000000000000001\n",
            "Epoch 143 loss: 8.520730972290039, F1: 0.4000000000000001\n",
            "Epoch 144 loss: 8.520009994506836, F1: 0.375\n",
            "Epoch 145 loss: 8.519294738769531, F1: 0.375\n",
            "Epoch 146 loss: 8.518587112426758, F1: 0.375\n",
            "Epoch 147 loss: 8.517887115478516, F1: 0.375\n",
            "Epoch 148 loss: 8.517193794250488, F1: 0.375\n",
            "Epoch 149 loss: 8.516507148742676, F1: 0.375\n",
            "Epoch 150 loss: 8.515829086303711, F1: 0.375\n",
            "Epoch 151 loss: 8.515155792236328, F1: 0.375\n",
            "Epoch 152 loss: 8.514490127563477, F1: 0.4000000000000001\n",
            "Epoch 153 loss: 8.513830184936523, F1: 0.4000000000000001\n",
            "Epoch 154 loss: 8.513177871704102, F1: 0.4000000000000001\n",
            "Epoch 155 loss: 8.512531280517578, F1: 0.4000000000000001\n",
            "Epoch 156 loss: 8.51189136505127, F1: 0.4000000000000001\n",
            "Epoch 157 loss: 8.51125717163086, F1: 0.4000000000000001\n",
            "Epoch 158 loss: 8.51063060760498, F1: 0.4000000000000001\n",
            "Epoch 159 loss: 8.510007858276367, F1: 0.4000000000000001\n",
            "Epoch 160 loss: 8.509392738342285, F1: 0.4000000000000001\n",
            "Epoch 161 loss: 8.508783340454102, F1: 0.4000000000000001\n",
            "Epoch 162 loss: 8.508179664611816, F1: 0.4000000000000001\n",
            "Epoch 163 loss: 8.507580757141113, F1: 0.4000000000000001\n",
            "Epoch 164 loss: 8.506988525390625, F1: 0.4000000000000001\n",
            "Epoch 165 loss: 8.506402015686035, F1: 0.4000000000000001\n",
            "Epoch 166 loss: 8.505820274353027, F1: 0.4000000000000001\n",
            "Epoch 167 loss: 8.505245208740234, F1: 0.4000000000000001\n",
            "Epoch 168 loss: 8.504674911499023, F1: 0.4000000000000001\n",
            "Epoch 169 loss: 8.504109382629395, F1: 0.4000000000000001\n",
            "Epoch 170 loss: 8.503549575805664, F1: 0.4000000000000001\n",
            "Epoch 171 loss: 8.502994537353516, F1: 0.4000000000000001\n",
            "Epoch 172 loss: 8.502446174621582, F1: 0.4000000000000001\n",
            "Epoch 173 loss: 8.501900672912598, F1: 0.4000000000000001\n",
            "Epoch 174 loss: 8.501361846923828, F1: 0.4000000000000001\n",
            "Epoch 175 loss: 8.500825881958008, F1: 0.4000000000000001\n",
            "Epoch 176 loss: 8.500297546386719, F1: 0.4000000000000001\n",
            "Epoch 177 loss: 8.499771118164062, F1: 0.4000000000000001\n",
            "Epoch 178 loss: 8.499251365661621, F1: 0.4000000000000001\n",
            "Epoch 179 loss: 8.498735427856445, F1: 0.4000000000000001\n",
            "Epoch 180 loss: 8.498224258422852, F1: 0.425\n",
            "Epoch 181 loss: 8.49771785736084, F1: 0.425\n",
            "Epoch 182 loss: 8.497215270996094, F1: 0.425\n",
            "Epoch 183 loss: 8.496718406677246, F1: 0.425\n",
            "Epoch 184 loss: 8.496224403381348, F1: 0.425\n",
            "Epoch 185 loss: 8.495736122131348, F1: 0.425\n",
            "Epoch 186 loss: 8.495250701904297, F1: 0.425\n",
            "Epoch 187 loss: 8.494771003723145, F1: 0.425\n",
            "Epoch 188 loss: 8.494294166564941, F1: 0.425\n",
            "Epoch 189 loss: 8.493821144104004, F1: 0.425\n",
            "Epoch 190 loss: 8.493352890014648, F1: 0.425\n",
            "Epoch 191 loss: 8.492889404296875, F1: 0.425\n",
            "Epoch 192 loss: 8.49242877960205, F1: 0.425\n",
            "Epoch 193 loss: 8.491971969604492, F1: 0.425\n",
            "Epoch 194 loss: 8.491519927978516, F1: 0.425\n",
            "Epoch 195 loss: 8.491069793701172, F1: 0.45\n",
            "Epoch 196 loss: 8.490625381469727, F1: 0.45\n",
            "Epoch 197 loss: 8.490184783935547, F1: 0.45\n",
            "Epoch 198 loss: 8.48974609375, F1: 0.45\n",
            "Epoch 199 loss: 8.489311218261719, F1: 0.45\n",
            "Epoch 200 loss: 8.488882064819336, F1: 0.45\n",
            "Epoch 201 loss: 8.488454818725586, F1: 0.45\n",
            "Epoch 202 loss: 8.488030433654785, F1: 0.45\n",
            "Epoch 203 loss: 8.487610816955566, F1: 0.45\n",
            "Epoch 204 loss: 8.48719310760498, F1: 0.45\n",
            "Epoch 205 loss: 8.486780166625977, F1: 0.45\n",
            "Epoch 206 loss: 8.486369132995605, F1: 0.45\n",
            "Epoch 207 loss: 8.4859619140625, F1: 0.45\n",
            "Epoch 208 loss: 8.48555850982666, F1: 0.45\n",
            "Epoch 209 loss: 8.485157012939453, F1: 0.45\n",
            "Epoch 210 loss: 8.484760284423828, F1: 0.45\n",
            "Epoch 211 loss: 8.484366416931152, F1: 0.45\n",
            "Epoch 212 loss: 8.48397445678711, F1: 0.45\n",
            "Epoch 213 loss: 8.483586311340332, F1: 0.45\n",
            "Epoch 214 loss: 8.483200073242188, F1: 0.45\n",
            "Epoch 215 loss: 8.482818603515625, F1: 0.45\n",
            "Epoch 216 loss: 8.482439994812012, F1: 0.45\n",
            "Epoch 217 loss: 8.482062339782715, F1: 0.45\n",
            "Epoch 218 loss: 8.481688499450684, F1: 0.45\n",
            "Epoch 219 loss: 8.481317520141602, F1: 0.45\n",
            "Epoch 220 loss: 8.480949401855469, F1: 0.45\n",
            "Epoch 221 loss: 8.480584144592285, F1: 0.45\n",
            "Epoch 222 loss: 8.48022174835205, F1: 0.45\n",
            "Epoch 223 loss: 8.47986125946045, F1: 0.45\n",
            "Epoch 224 loss: 8.479504585266113, F1: 0.45\n",
            "Epoch 225 loss: 8.479148864746094, F1: 0.45\n",
            "Epoch 226 loss: 8.47879695892334, F1: 0.45\n",
            "Epoch 227 loss: 8.478446960449219, F1: 0.45\n",
            "Epoch 228 loss: 8.478099822998047, F1: 0.45\n",
            "Epoch 229 loss: 8.477755546569824, F1: 0.45\n",
            "Epoch 230 loss: 8.477413177490234, F1: 0.45\n",
            "Epoch 231 loss: 8.477073669433594, F1: 0.45\n",
            "Epoch 232 loss: 8.476736068725586, F1: 0.45\n",
            "Epoch 233 loss: 8.476401329040527, F1: 0.45\n",
            "Epoch 234 loss: 8.476069450378418, F1: 0.45\n",
            "Epoch 235 loss: 8.475739479064941, F1: 0.45\n",
            "Epoch 236 loss: 8.475411415100098, F1: 0.45\n",
            "Epoch 237 loss: 8.475085258483887, F1: 0.45\n",
            "Epoch 238 loss: 8.474761962890625, F1: 0.45\n",
            "Epoch 239 loss: 8.474440574645996, F1: 0.45\n",
            "Epoch 240 loss: 8.474122047424316, F1: 0.45\n",
            "Epoch 241 loss: 8.47380542755127, F1: 0.45\n",
            "Epoch 242 loss: 8.473490715026855, F1: 0.45\n",
            "Epoch 243 loss: 8.473177909851074, F1: 0.45\n",
            "Epoch 244 loss: 8.472867965698242, F1: 0.45\n",
            "Epoch 245 loss: 8.472558975219727, F1: 0.45\n",
            "Epoch 246 loss: 8.47225284576416, F1: 0.45\n",
            "Epoch 247 loss: 8.471949577331543, F1: 0.45\n",
            "Epoch 248 loss: 8.471646308898926, F1: 0.45\n",
            "Epoch 249 loss: 8.471345901489258, F1: 0.45\n",
            "Epoch 250 loss: 8.471048355102539, F1: 0.45\n",
            "Epoch 251 loss: 8.470751762390137, F1: 0.45\n",
            "Epoch 252 loss: 8.470457077026367, F1: 0.45\n",
            "Epoch 253 loss: 8.47016429901123, F1: 0.45\n",
            "Epoch 254 loss: 8.469873428344727, F1: 0.45\n",
            "Epoch 255 loss: 8.469585418701172, F1: 0.45\n",
            "Epoch 256 loss: 8.469298362731934, F1: 0.45\n",
            "Epoch 257 loss: 8.469012260437012, F1: 0.45\n",
            "Epoch 258 loss: 8.468729972839355, F1: 0.45\n",
            "Epoch 259 loss: 8.468448638916016, F1: 0.45\n",
            "Epoch 260 loss: 8.468168258666992, F1: 0.45\n",
            "Epoch 261 loss: 8.467890739440918, F1: 0.45\n",
            "Epoch 262 loss: 8.467613220214844, F1: 0.45\n",
            "Epoch 263 loss: 8.467338562011719, F1: 0.45\n",
            "Epoch 264 loss: 8.467065811157227, F1: 0.45\n",
            "Epoch 265 loss: 8.46679401397705, F1: 0.45\n",
            "Epoch 266 loss: 8.466525077819824, F1: 0.45\n",
            "Epoch 267 loss: 8.466257095336914, F1: 0.45\n",
            "Epoch 268 loss: 8.46599006652832, F1: 0.45\n",
            "Epoch 269 loss: 8.465725898742676, F1: 0.45\n",
            "Epoch 270 loss: 8.465462684631348, F1: 0.45\n",
            "Epoch 271 loss: 8.465201377868652, F1: 0.45\n",
            "Epoch 272 loss: 8.46494197845459, F1: 0.45\n",
            "Epoch 273 loss: 8.464682579040527, F1: 0.45\n",
            "Epoch 274 loss: 8.464425086975098, F1: 0.45\n",
            "Epoch 275 loss: 8.464170455932617, F1: 0.45\n",
            "Epoch 276 loss: 8.463916778564453, F1: 0.45\n",
            "Epoch 277 loss: 8.463663101196289, F1: 0.45\n",
            "Epoch 278 loss: 8.46341323852539, F1: 0.45\n",
            "Epoch 279 loss: 8.463163375854492, F1: 0.45\n",
            "Epoch 280 loss: 8.46291446685791, F1: 0.45\n",
            "Epoch 281 loss: 8.462667465209961, F1: 0.45\n",
            "Epoch 282 loss: 8.462422370910645, F1: 0.45\n",
            "Epoch 283 loss: 8.462178230285645, F1: 0.45\n",
            "Epoch 284 loss: 8.461935997009277, F1: 0.45\n",
            "Epoch 285 loss: 8.461694717407227, F1: 0.45\n",
            "Epoch 286 loss: 8.461454391479492, F1: 0.45\n",
            "Epoch 287 loss: 8.461216926574707, F1: 0.45\n",
            "Epoch 288 loss: 8.460979461669922, F1: 0.45\n",
            "Epoch 289 loss: 8.460742950439453, F1: 0.45\n",
            "Epoch 290 loss: 8.460508346557617, F1: 0.45\n",
            "Epoch 291 loss: 8.460275650024414, F1: 0.45\n",
            "Epoch 292 loss: 8.460041999816895, F1: 0.45\n",
            "Epoch 293 loss: 8.45981216430664, F1: 0.45\n",
            "Epoch 294 loss: 8.459583282470703, F1: 0.45\n",
            "Epoch 295 loss: 8.459354400634766, F1: 0.45\n",
            "Epoch 296 loss: 8.459126472473145, F1: 0.45\n",
            "Epoch 297 loss: 8.458901405334473, F1: 0.45\n",
            "Epoch 298 loss: 8.4586763381958, F1: 0.45\n",
            "Epoch 299 loss: 8.458452224731445, F1: 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWEsvsBd5Zsd",
        "outputId": "bc45fe18-8c63-4374-d655-629d3485d1fa"
      },
      "source": [
        "# Score on the test set\n",
        "# Use the weights learned and stored in variable weights, run it on test set\n",
        "\n",
        "with torch.no_grad(): # for speed - we no longer care about gradients\n",
        "    total_loss = 0\n",
        "    y_pred = None\n",
        "    for [batch_x, batch_y] in test_loader:\n",
        "        \n",
        "        output = weights(batch_x)\n",
        "        softmaxed_output = softmax_fn(output)\n",
        "        batch_loss = loss_fn(softmaxed_output, batch_y)\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        _, predicted_labels = torch.max(softmaxed_output.data, 1)\n",
        "        if y_pred is not None:\n",
        "            y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
        "        else:\n",
        "            y_pred = predicted_labels\n",
        "    \n",
        "    f1 = f1_score(y_test, y_pred, average='micro')\n",
        "    print(f'Test set loss: {total_loss.item()}, F1: {f1}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set loss: 2.1902618408203125, F1: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WNcvY0F5d9M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwHPN_yz5nIG"
      },
      "source": [
        "### Questions\n",
        "\n",
        "The above code implements a classifier (categorical output). How would we modify it to do regression (continuous output)?\n",
        "\n",
        "You will need:\n",
        "- MSE loss instead of softmax + cross-entropy loss\n",
        "- Change the labels in y_train and y_test to numerical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kzCHOFi58yr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}