{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 9: Text embeddings\n",
        "\n",
        "### Rasika Bhalerao\n",
        "\n",
        "### Agenda:\n",
        "- Intro to [assignment 9](https://github.com/MIDS-W207/coursework_2022/blob/main/Homework/09%20Embeddings%20for%20Text.ipynb)"
      ],
      "metadata": {
        "id": "PC8P44-00rT0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "e2OgJ3v30pZ8"
      },
      "outputs": [],
      "source": [
        "### FROM HW9 ###\n",
        "\n",
        "\n",
        "# Import the libraries we'll use below.\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style=\"darkgrid\")  # default style\n",
        "import plotly.graph_objs as plotly  # for interactive plots\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(path=\"imdb.npz\",\n",
        "                                                      num_words=None,\n",
        "                                                      skip_top=0,\n",
        "                                                      maxlen=None,\n",
        "                                                      seed=113,\n",
        "                                                      start_char=1,\n",
        "                                                      oov_char=2,\n",
        "                                                      index_from=3)\n",
        "\n",
        "# The imdb dataset comes with an index mapping words to integers.\n",
        "# In the index the words are ordered by frequency they occur.\n",
        "index = imdb.get_word_index()\n",
        "\n",
        "# Because we used index_from=3 (above), setting aside ids below 3 for special\n",
        "# symbols, we need to add 3 to the index values.\n",
        "index = dict([(key, value+3) for (key, value) in index.items()])\n",
        "\n",
        "# Create a reverse index so we can lookup tokens assigned to each id.\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
        "reverse_index[1] = '<START>'  # start of input\n",
        "reverse_index[2] = '#'        # out-of-vocabulary (OOV)\n",
        "reverse_index[3] = '<UNUSED>'\n",
        "\n",
        "max_id = max(reverse_index.keys())\n",
        "\n",
        "def decode(token_ids):\n",
        "  \"\"\"Return a string with the decoded text given a list of token ids.\"\"\"\n",
        "  # Try looking up each id in the index, but return '#' (for OOV) if not found.\n",
        "  tokens = [reverse_index.get(i, \"#\") for i in token_ids]\n",
        "\n",
        "  # Connect the string tokens with a space.\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "\n",
        "def pad_data(sequences, max_length):\n",
        "  # Keras has a convenient utility for padding a sequence.\n",
        "  # Also make sure we get a numpy array rather than an array of lists.\n",
        "  return np.array(list(\n",
        "      tf.keras.preprocessing.sequence.pad_sequences(\n",
        "          sequences, maxlen=max_length, padding='post', value=0)))\n",
        "\n",
        "# Pad and truncate to 300 tokens.\n",
        "X_train_padded = pad_data(X_train, max_length=300)\n",
        "\n",
        "\n",
        "def limit_vocab(sequences, max_token_id, oov_id=2):\n",
        "  \"\"\"Replace token ids greater than or equal to max_token_id with the oov_id.\"\"\"\n",
        "  reduced_sequences = np.copy(sequences)\n",
        "  reduced_sequences[reduced_sequences >= max_token_id] = oov_id\n",
        "  return reduced_sequences\n",
        "\n",
        "# Reduce vocabulary to 1000 tokens.\n",
        "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
        "\n",
        "# Keras has a util to create one-hot encodings.\n",
        "X_train_padded = pad_data(X_train, max_length=20)\n",
        "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
        "X_train_one_hot = tf.keras.utils.to_categorical(X_train_reduced)\n",
        "\n",
        "\n",
        "\n",
        "def build_embeddings_model(average_over_positions=False,\n",
        "                           vocab_size=1000,\n",
        "                           sequence_length=20,\n",
        "                           embedding_dim=2):\n",
        "  \"\"\"Build a tf.keras model using embeddings.\"\"\"\n",
        "  # Clear session and remove randomness.\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(\n",
        "      input_dim=vocab_size,\n",
        "      output_dim=embedding_dim,\n",
        "      input_length=sequence_length)\n",
        "  )\n",
        "\n",
        "  if average_over_positions:\n",
        "    # This layer averages over the first dimension of the input by default.\n",
        "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "  else:\n",
        "    # Concatenate.\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=1,                     # output dim (for binary classification)\n",
        "      activation='sigmoid'         # apply the sigmoid function!\n",
        "  ))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', \n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFjlDWdV25ng",
        "outputId": "cd782b6f-3e7d-435e-a6c3-3e5ca1848718"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 50\n",
        "vocab_size = 100000\n",
        "\n",
        "model = build_embeddings_model(average_over_positions=True,\n",
        "                               vocab_size=vocab_size,\n",
        "                               sequence_length=20,\n",
        "                               embedding_dim=embedding_size)\n",
        "\n",
        "embeddings = model.layers[0].get_weights()[0]"
      ],
      "metadata": {
        "id": "BEDwPUAV6ti6"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [reverse_index[i] for i in range(1, 100)] # tokens for first 100 words\n",
        "print(tokens[10:20])\n",
        "print([index[word] for word in tokens[10:20]]) # index of each of those 10 words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBZ-ZivY7anC",
        "outputId": "7cf1ba3b-4e35-4f15-b8e6-c57b10a98a61"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['in', 'it', 'i', 'this', 'that', 'was', 'as', 'for', 'with', 'movie']\n",
            "[11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reverse_index[5]) # the 5th word\n",
        "embeddings[5] # embedding for 5th word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va6GxbfB6w8S",
        "outputId": "2dd3889f-607b-4e7d-ff18-16f6d655da5c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.02587608,  0.0494658 ,  0.02971274,  0.03185178,  0.01259113,\n",
              "       -0.00535594,  0.03925725,  0.03887851,  0.04810688,  0.0481171 ,\n",
              "        0.03332932, -0.0170391 , -0.04294174, -0.00588974, -0.01037066,\n",
              "        0.036463  , -0.02343742,  0.02986176, -0.01945804, -0.00665743,\n",
              "        0.02225571, -0.01584575, -0.01891565, -0.02592481, -0.0365484 ,\n",
              "        0.02132643,  0.00816028,  0.02198832,  0.01219358,  0.0487073 ,\n",
              "       -0.01504239, -0.01183355, -0.0479692 , -0.03535795, -0.03271524,\n",
              "       -0.04155707, -0.0199154 , -0.03582253, -0.01198497, -0.04797846,\n",
              "       -0.00686462, -0.02199868,  0.00723056,  0.02404376,  0.03393928,\n",
              "        0.01577142, -0.01272587, -0.02415756,  0.0410876 ,  0.02247739],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q-I_14d7Z-R",
        "outputId": "b300d988-3d57-40aa-ff0e-19d48c3a1afe"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial"
      ],
      "metadata": {
        "id": "gpORk7_69j6x"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word1 = 'number'\n",
        "word2 = 'data'\n",
        "\n",
        "embedding1 = embeddings[index[word1]]\n",
        "embedding2 = embeddings[index[word2]]\n",
        "# print(f\"{word1}: {embedding1}\")\n",
        "# print(f\"{word2}: {embedding2}\")\n",
        "print(f\"Cosine similarity: {1 - spatial.distance.cosine(embedding1, embedding2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwmNW_1E8IIE",
        "outputId": "1520dd09-2c1e-499c-bd32-6715f6fd12f0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity: -0.04787294566631317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JXxwLzmG82yu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}